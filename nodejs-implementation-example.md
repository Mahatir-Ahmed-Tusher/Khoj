# Node.js Implementation Example

‡¶è‡¶á ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ‡¶ü‡¶ø Node.js ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡¶æ‡¶Ø‡¶º‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶®‡¶ø‡¶Æ‡ßç‡¶®‡¶≤‡¶ø‡¶ñ‡¶ø‡¶§ ‡¶ï‡ßã‡¶° ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®:

## package.json Dependencies

```json
{
  "dependencies": {
    "node-fetch": "^3.3.2",
    "cheerio": "^1.0.0-rc.12",
    "jsdom": "^23.0.1",
    "p-queue": "^7.4.1",
    "playwright": "^1.40.0"
  }
}
```

## Main Implementation

```javascript
import fetch from 'node-fetch'
import * as cheerio from 'cheerio'
import { JSDOM } from 'jsdom'
import PQueue from 'p-queue'

// ALLOWED_SITES configuration
const ALLOWED_SITES = [
  // Bengali Fact-Checking Websites (HIGHEST PRIORITY)
  "rumorscanner.com",
  "fact-watch.org", 
  "boombangladesh.com",
  "factcheck.afp.com",
  "bssnews.net",
  "jachai.org",
  "bdfactcheck.com",
  "dismislab.com",
  "bangla.altnews.in",
  "bangla.factcrescendo.com",
  "bangla.vishvasnews.com",
  
  // Bangladeshi Mainstream News Sites
  "prothomalo.com",
  "bd-pratidin.com",
  "jugantor.com",
  "kalerkantho.com",
  "samakal.com",
  "thedailystar.net",
  "bdnews24.com",
  "dhakatribune.com",
  "thefinancialexpress.com.bd",
  "newagebd.net",
  "daily-sun.com",
  "theindependentbd.com",
  "bangladeshpost.net",
  "observerbd.com",
  "banglanews24.com",
  "banglatribune.com",
  "dhakapost.com",
  "risingbd.com",
  "barta24.com",
  "dhakatimes24.com",
  "somoynews.tv",
  "jamuna.tv",
  "independent24.com",
  "channel24bd.tv",
  "dbcnews.tv",
  "ntvbd.com",
  "rtvonline.com",
  "mzamin.com",
  "sangbad.net.bd",
  "jaijaidinbd.com",
  "bhorerkagoj.com",
  "dailyinqilab.com",
  "nayadiganta.com",
  "dainikazadi.net",
  "purbokone.net",
  "sylhetexpress.com",
  "khulnatimes.com",
  "amaderbarisal.com",
  "rajshahinews24.com",
  
  // International Sources
  "snopes.com",
  "politifact.com",
  "factcheck.org",
  "reuters.com",
  "apnews.com",
  "bbc.com",
  "fullfact.org",
  "washingtonpost.com",
  "bellingcat.com",
  "euvsdisinfo.eu",
  
  // Science & Health
  "healthfeedback.org",
  "sciencefeedback.co",
  "nasa.gov",
  "skepticalinquirer.org",
  "quackwatch.org",
  "sciencebee.com.bd",
  "bijnan-o-bijnani.co.in",
  "bigganblog.org",
  "boomlive.in",
  "thequint.com",
  "altnews.in",
  "vishvasnews.com",
  "newschecker.in",
  "cdc.gov",
  "who.int",
  "mayo.edu",
  "sciencenews.org",
  "livescience.com",
  "nationalgeographic.com",
  "theconversation.com",
  "factmyth.com",
  "leadstories.com",
  "checkyourfact.com"
]

// Helper functions
const normalizeUrl = (url) => {
  try {
    const urlObj = new URL(url.startsWith('http') ? url : `https://${url}`)
    return urlObj.origin + urlObj.pathname.replace(/\/+$/, '')
  } catch {
    return url
  }
}

const extractDomain = (url) => {
  try {
    return new URL(url.startsWith('http') ? url : `https://${url}`).hostname
  } catch {
    return url
  }
}

const isAllowedSite = (url) => {
  const domain = extractDomain(url)
  return ALLOWED_SITES.some(site => domain.includes(site))
}

// Fetch HTML with timeout
async function fetchHTML(url, timeoutMs = 10000) {
  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs)
  
  try {
    const response = await fetch(url, {
      signal: controller.signal,
      headers: {
        'User-Agent': 'Mozilla/5.0 (compatible; Khoj-FactChecker/1.0)',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
      }
    })
    
    clearTimeout(timeoutId)
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status} for ${url}`)
    }
    
    return await response.text()
  } catch (error) {
    clearTimeout(timeoutId)
    throw error
  }
}

// Extract main content from HTML
function extractMainContent(html, url) {
  try {
    // Simple content extraction using regex and basic parsing
    const titleMatch = html.match(/<title[^>]*>([^<]+)<\/title>/i)
    const title = titleMatch ? titleMatch[1].trim() : ''
    
    // Extract published date from various meta tags
    const publishedMatch = html.match(/<meta[^>]*property="article:published_time"[^>]*content="([^"]+)"/i) ||
                          html.match(/<time[^>]*datetime="([^"]+)"/i) ||
                          html.match(/<meta[^>]*name="date"[^>]*content="([^"]+)"/i)
    const published = publishedMatch ? publishedMatch[1] : null
    
    // Extract author
    const authorMatch = html.match(/<meta[^>]*name="author"[^>]*content="([^"]+)"/i) ||
                       html.match(/<meta[^>]*property="article:author"[^>]*content="([^"]+)"/i)
    const author = authorMatch ? authorMatch[1] : null
    
    // Extract main text content (simplified approach)
    const bodyMatch = html.match(/<body[^>]*>([\s\S]*?)<\/body>/i)
    if (!bodyMatch) return { title, published, author, text: '' }
    
    let bodyContent = bodyMatch[1]
    
    // Remove scripts and styles
    bodyContent = bodyContent.replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
    bodyContent = bodyContent.replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
    
    // Extract text from paragraphs, headings, and divs
    const textElements = bodyContent.match(/<(p|h[1-6]|div)[^>]*>([^<]+)<\/\1>/gi) || []
    const text = textElements
      .map(el => el.replace(/<[^>]*>/g, '').trim())
      .filter(text => text.length > 20) // Filter out very short texts
      .slice(0, 20) // Limit to first 20 elements
      .join(' ')
    
    return { title, published, author, text }
  } catch (error) {
    return { title: '', published: null, author: null, text: '' }
  }
}

// Calculate relevance score
function calculateRelevanceScore(content, query) {
  const queryLower = query.toLowerCase()
  const titleLower = content.title.toLowerCase()
  const textLower = content.text.toLowerCase()
  
  let score = 0
  
  // Title matches get higher weight
  if (titleLower.includes(queryLower)) score += 0.4
  if (textLower.includes(queryLower)) score += 0.3
  
  // Keyword matching
  const keywords = queryLower.split(/\s+/).filter(k => k.length > 2)
  keywords.forEach(keyword => {
    if (titleLower.includes(keyword)) score += 0.1
    if (textLower.includes(keyword)) score += 0.05
  })
  
  return Math.min(1, score)
}

// Find search candidates from a domain
async function findSearchCandidates(domain, query) {
  const candidates = new Set()
  const encodedQuery = encodeURIComponent(query)
  
  // Common search patterns
  const searchPatterns = [
    `https://${domain}/search?q=${encodedQuery}`,
    `https://${domain}/?s=${encodedQuery}`,
    `https://${domain}/find?q=${encodedQuery}`,
    `https://${domain}/search?query=${encodedQuery}`,
    `https://${domain}/?search=${encodedQuery}`,
    `https://${domain}/feed`,
    `https://${domain}/rss`,
    `https://${domain}/sitemap.xml`,
    `https://${domain}/news`,
    `https://${domain}/blog`,
    `https://${domain}/articles`
  ]
  
  for (const pattern of searchPatterns) {
    try {
      const html = await fetchHTML(pattern, 5000)
      
      // Extract links from the page
      const linkMatches = html.match(/href="([^"]+)"/gi) || []
      linkMatches.forEach(match => {
        const href = match.replace(/href="([^"]+)"/i, '$1')
        if (href && !href.startsWith('http')) {
          const fullUrl = href.startsWith('/') ? `https://${domain}${href}` : `https://${domain}/${href}`
          candidates.add(fullUrl)
        }
      })
      
      // Extract from RSS/XML
      const itemMatches = html.match(/<loc>([^<]+)<\/loc>/gi) || []
      itemMatches.forEach(match => {
        const url = match.replace(/<loc>([^<]+)<\/loc>/i, '$1')
        candidates.add(url)
      })
      
    } catch (error) {
      // Continue with next pattern
    }
  }
  
  return Array.from(candidates).slice(0, 50) // Limit to 50 candidates
}

// Main function to gather results from allowed sites
async function gatherFromAllowedSites(query) {
  const results = []
  const processedUrls = new Set()
  
  console.log(`üîç Searching ${ALLOWED_SITES.length} allowed sites for: "${query}"`)
  
  // Process sites in batches to avoid overwhelming
  const batchSize = 5
  for (let i = 0; i < ALLOWED_SITES.length; i += batchSize) {
    const batch = ALLOWED_SITES.slice(i, i + batchSize)
    
    await Promise.all(batch.map(async (domain) => {
      try {
        const candidates = await findSearchCandidates(domain, query)
        
        for (const url of candidates) {
          if (processedUrls.has(url)) continue
          processedUrls.add(url)
          
          try {
            const html = await fetchHTML(url, 8000)
            const content = extractMainContent(html, url)
            const score = calculateRelevanceScore(content, query)
            
            if (score >= 0.3) { // Minimum relevance threshold
              results.push({
                url: normalizeUrl(url),
                domain: extractDomain(url),
                title: content.title || 'No title',
                published: content.published,
                author: content.author,
                relevance_score: score,
                excerpt: content.text.substring(0, 200) + '...',
                source: 'allowed_sites'
              })
            }
          } catch (error) {
            // Skip this URL and continue
          }
        }
      } catch (error) {
        console.error(`Error processing domain ${domain}:`, error)
      }
    }))
    
    // Small delay between batches
    await new Promise(resolve => setTimeout(resolve, 1000))
  }
  
  // Sort by relevance score and return top results
  return results
    .sort((a, b) => b.relevance_score - a.relevance_score)
    .slice(0, 15)
}

// Fallback function using Tavily API
async function tavilyFallback(query) {
  try {
    const apiKey = process.env.TAVILY_API_KEY
    if (!apiKey) return []
    
    const response = await fetch("https://api.tavily.com/search", {
      method: "POST",
      headers: { 
        "content-type": "application/json", 
        "authorization": `Bearer ${apiKey}` 
      },
      body: JSON.stringify({ 
        query, 
        search_depth: "advanced", 
        max_results: 10 
      })
    })
    
    if (!response.ok) return []
    
    const data = await response.json()
    
    // Map to our schema
    return (data.results || []).map(r => ({
      url: r.url,
      domain: extractDomain(r.url),
      title: r.title || 'No title',
      published: r.published_date || null,
      author: null,
      relevance_score: r.score || 0.5,
      excerpt: (r.snippet || r.content || '').substring(0, 200) + '...',
      source: 'tavily_fallback'
    }))
  } catch (error) {
    console.error('Tavily fallback error:', error)
    return []
  }
}

// Main fact checking function
async function performFactCheck(query) {
  console.log(`üöÄ Starting domain-first fact check for: "${query}"`)

  // Step 1: Search in ALLOWED_SITES first
  const allowedSitesResults = await gatherFromAllowedSites(query)
  console.log(`‚úÖ Found ${allowedSitesResults.length} results from allowed sites`)

  let finalResults = allowedSitesResults
  let usedTavily = false
  let status = 'success'
  const notes = []

  // Step 2: Check if we need fallback
  if (allowedSitesResults.length < 3 || allowedSitesResults.every(r => r.relevance_score < 0.5)) {
    console.log('‚ö†Ô∏è Insufficient results from allowed sites, using Tavily fallback')
    const tavilyResults = await tavilyFallback(query)
    finalResults = [...allowedSitesResults, ...tavilyResults]
    usedTavily = true
    notes.push('Tavily API was used as fallback due to insufficient results from allowed sites')
    
    if (finalResults.length === 0) {
      status = 'no_results'
    } else if (finalResults.length < 5) {
      status = 'partial'
    }
  }

  // Sort final results by relevance
  finalResults.sort((a, b) => b.relevance_score - a.relevance_score)
  const topResults = finalResults.slice(0, 12)

  console.log(`üìä Final results: ${topResults.length} sources (${usedTavily ? 'with Tavily' : 'allowed sites only'})`)

  // Step 3: Generate report with AI (example prompt)
  const aiPrompt = `
**‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ö‡ßá‡¶ï‡¶ø‡¶Ç ‡¶Ö‡¶®‡ßÅ‡¶∞‡ßã‡¶ß:**
‡¶¶‡¶æ‡¶¨‡¶ø: ${query}

**‡¶â‡¶™‡¶≤‡¶¨‡ßç‡¶ß ‡¶â‡ßé‡¶∏‡¶∏‡¶Æ‡ßÇ‡¶π (${topResults.length}‡¶ü‡¶ø):**
${topResults.map((result, index) => `
**‡¶â‡ßé‡¶∏ ${index + 1}** (${result.source === 'allowed_sites' ? '‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∏‡¶æ‡¶á‡¶ü' : 'Tavily API'}):
‡¶∂‡¶ø‡¶∞‡ßã‡¶®‡¶æ‡¶Æ: ${result.title}
‡¶°‡ßã‡¶Æ‡ßá‡¶á‡¶®: ${result.domain}
URL: ${result.url}
‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï‡¶§‡¶æ ‡¶∏‡ßç‡¶ï‡ßã‡¶∞: ${result.relevance_score.toFixed(2)}
‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ: ${result.published || '‡¶Ö‡¶ú‡¶æ‡¶®‡¶æ'}
‡¶≤‡ßá‡¶ñ‡¶ï: ${result.author || '‡¶Ö‡¶ú‡¶æ‡¶®‡¶æ'}
‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡¶ø‡¶™‡ßç‡¶§ ‡¶¨‡¶ø‡¶¨‡¶∞‡¶£: ${result.excerpt}
`).join('\n')}

**‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:**
‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û ‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü ‡¶ö‡ßá‡¶ï‡¶æ‡¶∞‡•§ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶¶‡¶æ‡¶¨‡¶ø‡¶ü‡¶ø ‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®‡•§

**‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü‡ßá‡¶∞ ‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã:**
‡ßß. **‡¶¶‡¶æ‡¶¨‡¶ø:** [‡¶Æ‡ßÇ‡¶≤ ‡¶¶‡¶æ‡¶¨‡¶ø‡¶ü‡¶ø ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®]
‡ß®. **‡¶∏‡¶ø‡¶¶‡ßç‡¶ß‡¶æ‡¶®‡ßç‡¶§:** [‡¶∏‡¶§‡ßç‡¶Ø/‡¶Æ‡¶ø‡¶•‡ßç‡¶Ø‡¶æ/‡¶≠‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§‡¶ø‡¶Æ‡ßÇ‡¶≤‡¶ï/‡¶Ö‡¶Ø‡¶æ‡¶ö‡¶æ‡¶á‡¶ï‡ßÉ‡¶§] - ‡¶Ü‡¶§‡ßç‡¶Æ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶∏‡¶π
‡ß©. **‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£:** 
   - ‡¶¶‡¶æ‡¶¨‡¶ø‡¶∞ ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶¶‡¶ø‡¶ï ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶®
   - ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶â‡ßé‡¶∏‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
   - ‡¶¨‡¶ø‡¶∞‡ßã‡¶ß‡ßÄ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶§‡ßÅ‡¶≤‡ßá ‡¶ß‡¶∞‡ßÅ‡¶®
‡ß™. **‡¶™‡ßç‡¶∞‡¶Æ‡¶æ‡¶£‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®:**
   - ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶â‡ßé‡¶∏‡ßá‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§‡¶æ ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
   - ‡¶™‡ßç‡¶∞‡¶æ‡¶•‡¶Æ‡¶ø‡¶ï ‡¶â‡ßé‡¶∏ ‡¶¨‡¶®‡¶æ‡¶Æ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡¶ø‡¶ï ‡¶â‡ßé‡¶∏‡ßá‡¶∞ ‡¶™‡¶æ‡¶∞‡ßç‡¶•‡¶ï‡ßç‡¶Ø ‡¶ï‡¶∞‡ßÅ‡¶®
‡ß´. **‡¶∏‡¶§‡¶∞‡ßç‡¶ï‡¶§‡¶æ:** [‡¶Ø‡¶¶‡¶ø ‡¶•‡¶æ‡¶ï‡ßá]
‡ß¨. **‡¶â‡¶™‡¶∏‡¶Ç‡¶π‡¶æ‡¶∞:** [‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™]

**‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂‡¶®‡¶æ:**
- ‡¶∏‡¶¨‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®
- ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§‡¶ø ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶∞‡ßá‡¶´‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶® [‡ßß], [‡ß®], [‡ß©] ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø
- ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶Æ‡¶æ‡¶£-‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶π‡ßã‡¶®
- ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶â‡ßé‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶§‡¶•‡ßç‡¶Ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶Ö‡¶®‡ßÅ‡¶¨‡¶æ‡¶¶ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶¶‡¶æ‡¶¨‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶¶‡¶ø‡¶®
- ‡¶Ü‡¶§‡ßç‡¶Æ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ (‡ß¶-‡ßß‡ß¶‡ß¶%) ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßÅ‡¶®
- ‡¶Ø‡¶¶‡¶ø Tavily API ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡ßá‡¶ü‡¶æ ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶≠‡¶æ‡¶¨‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßÅ‡¶®
- **‡¶Æ‡¶π‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£:** ‡¶Ü‡¶™‡¶®‡¶ø ‡¶®‡¶ø‡¶ú‡ßá ‡¶•‡ßá‡¶ï‡ßá "‡¶â‡ßé‡¶∏‡ßá‡¶∞ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ" ‡¶¨‡¶æ "‡¶â‡ßé‡¶∏‡¶∏‡¶Æ‡ßÇ‡¶π" ‡¶∏‡ßá‡¶ï‡¶∂‡¶® ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§ ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶â‡¶™‡¶∞‡ßá ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶â‡ßé‡¶∏‡¶∏‡¶Æ‡ßÇ‡¶π ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®‡•§
- ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü‡ßá‡¶∞ ‡¶∂‡ßá‡¶∑‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶â‡ßé‡¶∏ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ ‡¶¶‡ßá‡¶¨‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶®‡ßá‡¶á‡•§
`

  return {
    status,
    used_tavily: usedTavily,
    selected_urls: topResults,
    notes,
    claim: query,
    ai_prompt: aiPrompt, // AI prompt for external processing
    searchStats: {
      totalSitesSearched: ALLOWED_SITES.length,
      totalResultsFound: finalResults.length,
      allowedSitesResults: allowedSitesResults.length,
      tavilyResults: usedTavily ? finalResults.length - allowedSitesResults.length : 0
    },
    generatedAt: new Date().toISOString()
  }
}

// Usage example
async function main() {
  const query = "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶ï‡¶∞‡ßã‡¶®‡¶æ ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏‡ßá‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® ‡¶≠‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶®‡ßç‡¶ü"
  
  try {
    const result = await performFactCheck(query)
    console.log('Fact check result:', JSON.stringify(result, null, 2))
  } catch (error) {
    console.error('Error:', error)
  }
}

// Run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main()
}

export { performFactCheck, ALLOWED_SITES }
